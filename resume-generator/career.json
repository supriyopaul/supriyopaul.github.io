{
  "$schema": "./schema.json",
  "title": "Supriyo Paul - Professional Journey",
  "personalDetails": {
    "fullName": "Supriyo Paul",
    "currentTitle": "Senior Software Engineer",
    "brandingStatements": [
        "Software Engineer specializing in Python development, DevOps, and Automation Engineering ‚öôÔ∏è",
        "Currently exploring the realm of knowledge graphs and LLM frameworks üß†",
        "Aspiring system architect üèóÔ∏è",
        "Passionate advocate for clean, maintainable code ‚ú®",
        "Tinkering with applibot.io my latest hobby project! ü§ñ",
        "Hobbies include Badminton, Coooking, sticker making, watching Anime, and Docuseries üé¨",
        "Favorite authors: Yuval Noah Harari and Paulo Coelho ‚úçÔ∏è"
    ],
    "profileImage": "https://drive.google.com/file/d/1TJEi5cIrhipklvjcGioX1noZtypNHKuH/view?usp=drive_link",
    "contact": {
      "email": "paul.supriyo.paul@gmail.com",
      "phone": "+91 8340265344",
      "location": "Whitefield, Bengaluru",
      "linkedin": "https://www.linkedin.com/in/supriyopaul95",
      "github": "https://github.com/supriyopaul",
      "website": "https://supriyopaul.github.io/"
    },
    "downloadLink": "https://github.com/supriyopaul/supriyopaul.github.io/blob/master/SupriyoPaul.pdf"
  },
  "timeline": [
    {
      "id": "exp-hm-2024", "type": "experience", "heading": "Software Engineer II",
      "organization": { "name": "H&M", "logoUrl": "https://drive.google.com/file/d/1JZ7aeGCQdHjZKfSsysuU-e6ag0yVFKyk/view?usp=drive_link"}, "role": "Software Engineer II",
      "startDate": "2024-03-01", "endDate": null,
      "narrative": "Using `applibot`, I secured my current role at H&M. I work on the central Hub, a platform that acts as a **configurable, low-code workflow** for handling data integrations between our \"Centric\" PLM system and **over 10+ downstream systems**. My responsibilities include owning multiple business objects, overseeing their integrations, and maintaining our core modules and ETL pipelines on **Google Cloud Platform (GCP)**, using services like **Cloud Run, GKE, and SpannerDB**. I am deeply involved in the entire SDLC, from solution design to production support. Outside of my technical responsibilities, I'm an active part of the company culture and a keen **badminton** player, having won several internal tournaments.",
      "resumePoints": [
        "Developing an integration platform to handle complex business objects to and from PLM software APIs and other in-house applications, making them available in a standardized format for over 10+ downstream systems.",
        "This involves creating a configurable, no-code workflow to streamline and automate the process."
      ],
      "skillsUsed": [
        {"category": "DEVOPS & TOOLS", "skills": ["Google Cloud Platform (GCP)", "GKE"]},
        {"category": "DATABASES", "skills": ["SpannerDB"]},
        {"category": "PROGRAMMING LANGUAGES", "skills": ["Python"]},
        {"category": "TECHNOLOGIES & CONCEPTS", "skills": ["PLM systems", "Enterprise Integration Patterns", "End-to-End SDLC Ownership"]}
      ],
      "personalInterests": ["Badminton"]
    },
    {
        "id": "project-applibot-2023",
        "type": "project",
        "heading": "applibot.io",
        "startDate": "2023-01-01",
        "endDate": "2024-01-01",
        "narrative": "To continuously sharpen my foundational skills, I dedicated time to competitive programming. To prepare, I took a dedicated course on **Data Structures & Algorithms** and also completed a comprehensive **[System Design certification](https://www.udemy.com/certificate/UC-a6688b24-e5c2-48e2-9c10-231f972eae68/)**. This preparation culminated in a **[50-day challenge on LeetCode](https://drive.google.com/file/d/1zZOSrQ-3x5oreK_nLA0wplgx-I7muvmc/view?usp=sharing)**, where I earned a badge awarded to the **top 6.9% of global participants**.\n\nFueled by my experience across diverse teams and my ambition as an **aspiring system architect**, I sought a role with greater end-to-end ownership. To aid my job search, I built **`applibot.io`**, a **RAG application** to automate job applications. The tech stack included **LanceDB**, **PostgreSQL**, **FastAPI**, and I leveraged **LangChain** alongside OpenAI models as the primary LLM framework.",
        "skillsUsed": [
            {"category": "TECHNOLOGIES & CONCEPTS", "skills": ["RAG Architecture", "LLMs", "LangChain", "Full-Stack Development", "Product Ideation", "FastAPI"]},
            {"category": "DATABASES", "skills": ["Vector Databases"]}
        ],
        "certifications": [
            { "name": "Data Structures & Algorithms", "issuer": "Udemy", "certificateURL": "https://www.udemy.com/certificate/UC-e7d015c6-9477-45e0-9e0c-8d1863d038f6/"},
            { "name": "System Design", "issuer": "Udemy", "certificateURL": "https://www.udemy.com/certificate/UC-a6688b24-e5c2-48e2-9c10-231f972eae68/" },
            { "name": "Top 6.9% Badge 2022", "issuer": "LeetCode", "certificateURL": "https://drive.google.com/file/d/1zZOSrQ-3x5oreK_nLA0wplgx-I7muvmc/view?usp=sharing" }
        ],
        "personalInterests": ["Anime", "Cooking"],
        "resumePoints": [
            "Built a RAG application to automate job applications, leveraging LanceDB, PostgreSQL, and FastAPI.",
            "Utilized LangChain and OpenAI models as the primary LLM framework."
        ]
    },
    {
      "id": "exp-jio-2021", "type": "experience", "heading": "Software Engineer II",
      "organization": { "name": "Jio Platforms Private Limited", "logoUrl": "https://drive.google.com/file/d/1OLLKlc-sFAXQrqcbv8IqA4tv_JENV-IH/view?usp=drive_link"}, "role": "Software Engineer II",
      "startDate": "2021-09-01", "endDate": "2024-03-01",
      "narrative": "I transitioned to the AI-Center of Excellence (AI-COE) team at Jio. The team's vision was to build a unified **ML and MLOps platform** to standardize pipelines for data ingestion, model training, and inference. To ground myself in the team's domain upon joining, I completed the **[AI for Everyone](https://www.coursera.org/account/accomplishments/certificate/JMQWSQBKDSZC)** course.\n\nMy primary contribution was engineering a comprehensive **Knowledge Graph platform** built on **ArangoDB**, using **Redis** for caching to enhance query performance. The platform's microservices architecture was built on **gRPC and Protocol Buffers**, for which I earned a specialized **[certification](https://www.udemy.com/certificate/UC-1dd3f3c2-d8ad-46b5-9a09-801f26cf0df6/)**. As the team grew, my role evolved to include mentoring and training new members. Beyond leadership, I also **spearheaded the performance monitoring, scaling, and refinement of our Knowledge Graph services**, ensuring high reliability and efficiency. The entire platform was deployed on **Kubernetes** using CI/CD pipelines I built on **Azure DevOps**.",
      "resumePoints": [
        "Engineered a comprehensive Knowledge-graph platform, encompassing micro-services like Schema, Data, and Search, to facilitate the semantic storage and retrieval of domain knowledge, enhancing data accessibility and interoperability.",
        "Spearheaded the performance monitoring, scaling, and refinement of Knowledge Graph services, ensuring optimal scalability for Big Data ingestion and retrieval, and enhancing service reliability and efficiency.",
        "Provided mentorship on best practices and maintainability within the AICOE's AI platform, fostering a culture of knowledge sharing and continuous improvement within the team."
      ],
      "skillsUsed": [
        {"category": "TECHNOLOGIES & CONCEPTS", "skills": ["System Design & Architecture", "gRPC", "Knowledge modelling"]},
        {"category": "DATABASES", "skills": ["ArangoDB", "Redis"]},
        {"category": "DEVOPS & TOOLS", "skills": ["Kubernetes", "Azure DevOps", "MLOps"]},
        {"category": "SOFT SKILLS", "skills": ["Technical Leadership & Mentorship"]}
      ],
      "certifications": [
        { "name": "AI for Everyone", "issuer": "Coursera", "certificateURL": "https://www.coursera.org/account/accomplishments/certificate/JMQWSQBKDSZC" },
        { "name": "Protocol Buffers", "issuer": "Udemy", "certificateURL": "https://www.udemy.com/certificate/UC-1dd3f3c2-d8ad-46b5-9a09-801f26cf0df6/" }
      ]
    },
    {
      "id": "exp-ajio-2019", "type": "experience", "heading": "DevOps Engineer",
      "organization": { "name": "Ajio", "logoUrl": "https://seeklogo.com/images/A/ajio-logo-E5B162A565-seeklogo.com.png"}, "role": "DevOps Engineer",
      "startDate": "2019-09-01", "endDate": "2021-09-01",
      "narrative": "I moved to Ajio, where I **founded and led the Tools & Automation team**. My mission was to implement strategic automation solutions that would streamline workflows and eliminate redundant tasks across the organization. This was a critical period, as Ajio was undergoing a massive migration to a microservices architecture.\n\nBefore building the large-scale ELK pipeline, my initial focus was on operational stability, where I implemented **Prometheus** for monitoring and alerting on ad-hoc internal tasks. My team's flagship project became a **centralized visibility platform** built on **ELK and Kafka clusters**, designed to pinpoint performance bottlenecks in real-time. We scaled this platform to support over **16 microservices**, processing over **7 billion log records daily**. I was responsible for the architecture, scalability, and optimization of these clusters, as well as leading the infrastructure benchmarking for migration to Jio‚Äôs in-house cloud, ‚ÄúJAWS‚Äù. All deployments and configuration changes were managed using **Ansible**.",
      "resumePoints": [
        "Engineered and deployed multiple robust clusters designed for extensive log aggregation, capable of processing over 7 billion records daily, alongside conducting comprehensive load testing for the in-house cloud infrastructure platform: JAWS.",
        "Oversaw and optimized the scalability of ELK and Kafka clusters to accommodate the diverse operational needs across various Jio business units, ensuring seamless data flow and reduced system latency.",
        "Founded and led the Tools & Automation team at Ajio, implementing strategic automation solutions to streamline workflows and eliminate redundant tasks across multiple departments, enhancing overall operational efficiency."
      ],
      "skillsUsed": [
        {"category": "DEVOPS & TOOLS", "skills": ["ELK Stack Administration", "Apache Kafka Management", "Prometheus", "Ansible", "Grafana", "Kibana", "Dashboarding"]},
        {"category": "TECHNOLOGIES & CONCEPTS", "skills": ["Large-Scale Data Architecture", "System Scalability & Performance Tuning", "Microservices Monitoring"]},
        {"category": "SOFT SKILLS", "skills": ["Team Leadership"]}
      ],
      "personalInterests": ["Book reading", "Stock Trading"]
    },
    {
      "id": "exp-deepcompute-2017", "type": "experience", "heading": "Associate Software Engineer",
      "organization": { "name": "Deepcompute LLC", "logoUrl": "https://drive.google.com/file/d/1eXmg-KJprHRKjKYBEZDCOW4Rs67UvY_5/view?usp=drive_link"}, "role": "Associate Software Engineer",
      "startDate": "2017-08-01", "endDate": "2019-09-01",
      "narrative": "My professional career began at **Deepcompute**, a dynamic startup that served as the primary technology backend for nference. Nference's platform was designed to accelerate **drug discovery and development by analyzing and organizing vast datasets** scraped from medical literature, Wikipedia, FDA databases, and clinical trials. For its clients, including major pharmaceutical corporations like Johnson & Johnson, Pfizer, and Merck, the platform produced powerful insights through tools like **gene-drug relationship heatmaps and trend analysis charts**.\n\nIn this role, I transitioned from Java to **Python** and was tasked with building and supporting critical internal tools. I actively **collaborated with cross-functional teams to develop and enhance several open-source libraries** and modules like `logagg`, `basescript`, and `deeputil`. My primary responsibilities revolved around the data pipeline, which involved ingesting large volumes of JSON data and administering the supporting infrastructure, including **MongoDB**, **InfluxDB**, and **NSQ**.\n\nBeyond tool development, I played a pivotal role in shaping our engineering practices by facilitating deployments using **Docker and Travis CI**. This role solidified my belief in robust engineering; I became a **passionate advocate for clean, maintainable code**, especially after implementing CI/CD checks and using tools like `doctest` to enforce quality and documentation simultaneously. After my first year, I also briefly ventured into creative design, making and selling stickers.",
      "resumePoints": [
        "Collaborated with cross-functional teams to develop and enhance open-source libraries and modules such as logagg, basescript, deeputil, contributing to the creation of versatile and reusable software components.",
        "Played a pivotal role in application development and facilitated teams in managing deployments and logs, establishing and enforcing stringent benchmarks for code quality to ensure the delivery of robust and efficient software solutions."
      ],
      "skillsUsed": [
        {"category": "PROGRAMMING LANGUAGES", "skills": ["Python", "Bash Scripting"]},
        {"category": "TECHNOLOGIES & CONCEPTS", "skills": ["Data Pipelines", "OOPS"]},
        {"category": "DEVOPS & TOOLS", "skills": ["NSQ", "Docker", "CI/CD Pipeline Management", "Git Workflows", "Linux System Administration", "Travis CI"]},
        {"category": "DATABASES", "skills": ["MongoDB", "InfluxDB"]},
        {"category": "SOFT SKILLS", "skills": ["Open-Source Contribution"]
      }],
      "personalInterests": ["Making stickers"]
    },
    {
      "id": "edu-mit-2013", "type": "education", "heading": "Bachelor of Technology, Computer Science",
      "organization": { "name": "Mallabhum Institute of Technology"}, "role": "Undergraduate Student",
      "startDate": "2013-08-01", "endDate": "2017-05-31",
      "narrative": "I chose to major in Computer Science and Engineering, pursuing my degree from **Mallabhum Institute of Technology**. This was a decision I was committed to, even though it was unconventional within my family at the time. My college years were a period of immense learning, where I was formally introduced to the core principles that govern the digital world. The curriculum covered a wide spectrum of essential topics, including **16-bit microprocessors, Computer Architecture, the Linux kernel, and intricate Networks and protocols**. This comprehensive education provided me with a robust understanding of how software and hardware interact.\n\nOutside of academics, I also discovered a passion for volleyball, where I honed my skills in teamwork, strategy, and coordination, eventually becoming a proficient player.",
      "resumePoints": [],
      "skillsUsed": [
        {"category": "TECHNOLOGIES & CONCEPTS", "skills": ["Core Computer Science fundamentals (Data Structures, Algorithms, OS)", "understanding of computer architecture and networking", "Linux kernel basics"]},
        {"category": "SOFT SKILLS", "skills": ["teamwork", "collaboration"]
      }],
      "personalInterests": ["Volleyball"]
    },
    {
      "id": "edu-dav-2011", "type": "education", "heading": "High School",
      "organization": { "name": "D.A.V Public School"}, "role": "Student",
      "startDate": "2011-04-01", "endDate": "2013-03-31",
      "narrative": "Continuing my education at DAV Public School, I naturally opted for Computer Science to delve deeper into my passion. This period was instrumental in broadening my technical horizons, as I learned additional programming languages, including **C++ and PHP**, becoming significantly more tech-savvy. Beyond the curriculum, I was captivated by the burgeoning smartphone ecosystem and spent countless hours on the **XDA Developers forums**. This hands-on exploration involved experimenting with custom firmware, software modifications, and troubleshooting complex technical issues, which sharpened my practical problem-solving skills. It was during this time that I solidified my ambition to pursue **Computer Science as a professional career**, with my passion for football serving as a cherished Plan B.",
      "resumePoints": [],
      "skillsUsed": [
        {"category": "PROGRAMMING LANGUAGES", "skills": ["C++", "PHP"]},
        {"category": "TECHNOLOGIES & CONCEPTS", "skills": ["advanced programming concepts", "mobile operating system fundamentals"]},
        {"category": "SOFT SKILLS", "skills": ["practical troubleshooting", "self-directed learning"]
      }],
      "personalInterests": ["XDA Developers forums", "Football"]
    },
    {
      "id": "edu-denobili-2000", "type": "education", "heading": "School",
      "organization": { "name": "De Nobili School"}, "role": "Student",
      "startDate": "2000-04-01", "endDate": "2011-03-31",
      "narrative": "My academic journey began at De Nobili School, where I spent eleven years building my foundational knowledge. It was here that my natural curiosity for technology first took root. I was deeply intrigued by the dawn of the digital age, particularly **the internet and its underlying mechanisms**. This fascination led me to enroll in Computer Science classes, which provided my first formal introduction to the world of programming through the **Java** language.",
      "resumePoints": [],
      "skillsUsed": [
        {"category": "PROGRAMMING LANGUAGES", "skills": ["introductory Java"]},
        {"category": "TECHNOLOGIES & CONCEPTS", "skills": ["Foundational programming logic", "analytical thinking"]
      }]
    }
  ]
}